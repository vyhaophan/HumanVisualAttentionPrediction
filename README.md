# Human Visual Attention Prediction

This is one of my school's projects. We wanted to give a try on re-building an Deep Learning model from an A-rank paper. The original paper is [Predicting Goal-Directed Human Attention Using Inverse Reinforcement Learning](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Predicting_Goal-Directed_Human_Attention_Using_Inverse_Reinforcement_Learning_CVPR_2020_paper.html) (or Scanpath Prediction for short). Surprisingly, our rebuilt model gets higher scores in comparison with the authors' in the paper.

The file [Report.pdf](https://github.com/vyhaophan/HumanVisualAttentionPrediction/blob/main/Report.pdf) is written in Vietnamese. It describes our process of applying our research and knowledge into coding. The file [Scanpath_Prediction (train + evaluate + save_images).ipynb](https://github.com/vyhaophan/HumanVisualAttentionPrediction/blob/main/Scanpath_Prediction%20(train%20%2B%20evaluate%20%2B%20save_images).ipynb) contains the source code I used for running the original model and also some other modifications. File [Scanpath_Ratio_Analysis.ipynb](https://github.com/vyhaophan/HumanVisualAttentionPrediction/blob/main/Scanpath_Ratio_Analysis.ipynb) and [MultiMatch_PhanTich.ipynb](https://github.com/vyhaophan/HumanVisualAttentionPrediction/blob/main/MultiMatch_PhanTich.ipynb) are source codes for evaluating model's results using 2 metrics Scanpath Ratio and MultiMatch.

Inverse Reinforcement Learning is such a whole new world for me. Thanks to this project, I have my first time dealing with a real IRL problems. It opened my eyes with how the AI can do and will do in the future. Something saved in my heart: how an IRL model works and many new evaluating metrics (MultiMatch, Scanpath Ratio,...)
